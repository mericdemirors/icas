{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helper_exceptions import *\n",
    "from helper_functions import write_clusters, print_verbose\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, HDBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DL_Datasets import *\n",
    "from DL_Models import *\n",
    "from DL_ModelTrainer import ModelTrainer\n",
    "\n",
    "dataset = ImageDataset(\"heredenene\")\n",
    "model = PowerOf2s256andAbove()\n",
    "mt = ModelTrainer(num_of_epochs=1, lr=0.001,\n",
    "                  batch_size=16, loss_type=\"mse\",\n",
    "                  dataset=dataset, model=model,\n",
    "                  ckpt_path=\"heredenene_PowerOf2s256andAbove_mse_05:16:19:01:53/min_loss:0.06544473022222519_epoch:19.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DL_Clustering():\n",
    "    def __init__(self, model_trainer, method, batch_size, number_of_clusters=[10], max_iter=[200], DBSCAN_eps=[0.5],\n",
    "                DBSCAN_min_samples=[5], HDBSCAN_min_cluster_size=[5], HDBSCAN_max_cluster_size=[None], option=\"\",\n",
    "                transfer=\"copy\", overwrite=False, verbose=0):\n",
    "        self.model_trainer = model_trainer\n",
    "        self.method = method\n",
    "        self.batch_size = batch_size\n",
    "        self.number_of_clusters = number_of_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.DBSCAN_eps = DBSCAN_eps\n",
    "        self.DBSCAN_min_samples = DBSCAN_min_samples\n",
    "        self.HDBSCAN_min_cluster_size = HDBSCAN_min_cluster_size\n",
    "        self.HDBSCAN_max_cluster_size = HDBSCAN_max_cluster_size    \n",
    "        self.option = option  \n",
    "        self.transfer = transfer\n",
    "        self.overwrite = overwrite\n",
    "        self.verbose = verbose  \n",
    "        \n",
    "        if self.option == \"merge\":\n",
    "            self.result_container_folder = self.model_trainer.dataset.root_dir\n",
    "        else:\n",
    "            base_folder, images_folder_name = os.path.split(self.model_trainer.dataset.root_dir)\n",
    "            self.result_container_folder = os.path.join(base_folder, images_folder_name + \"_clustered\")\n",
    "\n",
    "        self.arguman_check()\n",
    "    \n",
    "    def __str__(self, verbose=0):\n",
    "        \"\"\"casting to string method for printing/debugging object attributes\n",
    "\n",
    "        Returns:\n",
    "            str: object attribute information\n",
    "        \"\"\"\n",
    "        attributes = vars(self)\n",
    "        attr_strings = [f\"{key}: {value}\" for key, value in attributes.items()]\n",
    "        return \"-\"*70 + \"\\n\" + \"\\n\".join(attr_strings) + \"\\n\" + \"-\"*70\n",
    "\n",
    "    def arguman_check(self, verbose=0):\n",
    "        valid_methods = [\"kmeans\", \"hierarchy\", \"DBSCAN\", \"gaussian\", \"HDBSCAN\"]\n",
    "        if self.method not in valid_methods:\n",
    "            6/0\n",
    "        \n",
    "    def get_models(self, verbose=0):\n",
    "        def calculate_grid_search():\n",
    "            param_grid = []\n",
    "            if self.method == \"kmeans\":\n",
    "                param_grid = list(itertools.product(self.number_of_clusters, self.max_iter))\n",
    "            elif self.method == \"hierarchy\":\n",
    "                param_grid = self.number_of_clusters\n",
    "            elif self.method == \"DBSCAN\":\n",
    "                param_grid = list(itertools.product(self.DBSCAN_eps, self.DBSCAN_min_samples))\n",
    "            elif self.method == \"gaussian\":\n",
    "                param_grid = list(itertools.product(self.number_of_clusters, self.max_iter))\n",
    "            elif self.method == \"HDBSCAN\":\n",
    "                param_grid = list(itertools.product(self.HDBSCAN_min_cluster_size, self.HDBSCAN_max_cluster_size))\n",
    "            \n",
    "            return param_grid\n",
    "        param_grid = calculate_grid_search()\n",
    "\n",
    "        models = []\n",
    "        for params in param_grid:\n",
    "            if self.method == \"kmeans\":\n",
    "                n_clusters, max_iter = params\n",
    "                models.append(KMeans(n_clusters=n_clusters, max_iter=max_iter, verbose=verbose))\n",
    "            elif self.method == \"hierarchy\":\n",
    "                n_clusters = params\n",
    "                models.append(AgglomerativeClustering(n_clusters=n_clusters))\n",
    "            elif self.method == \"DBSCAN\":\n",
    "                eps, min_samples = params\n",
    "                models.append(DBSCAN(eps=eps, min_samples=min_samples))\n",
    "            elif self.method == \"gaussian\":\n",
    "                n_clusters, max_iter = params\n",
    "                models.append(GaussianMixture(n_components=n_clusters, max_iter=max_iter, verbose=verbose))\n",
    "            elif self.method == \"HDBSCAN\":\n",
    "                min_cluster_size, max_cluster_size = params\n",
    "                models.append(HDBSCAN(min_cluster_size=min_cluster_size, max_cluster_size=max_cluster_size))\n",
    "\n",
    "        return models\n",
    "\n",
    "    def find_best_model(self, models, reps, verbose=0):\n",
    "        silhouette_scores, db_scores, ch_scores = [], [], []\n",
    "        for model in tqdm(models):\n",
    "            print(model)\n",
    "            labels = model.fit_predict(reps)\n",
    "            \n",
    "            silhouette_scores.append(silhouette_score(reps, labels))\n",
    "            db_scores.append(davies_bouldin_score(reps, labels))\n",
    "            ch_scores.append(calinski_harabasz_score(reps, labels))\n",
    "\n",
    "        silhouette_scores, db_scores, ch_scores = np.array(silhouette_scores), np.array(db_scores), np.array(ch_scores)\n",
    "        silhouette_scores = (silhouette_scores - silhouette_scores.min())/(silhouette_scores.max()-silhouette_scores.min())\n",
    "        db_scores = (db_scores - db_scores.min())/(db_scores.max()-db_scores.min())\n",
    "        ch_scores = (ch_scores - ch_scores.min())/(ch_scores.max()-ch_scores.min())\n",
    "        \n",
    "        combined_scores = silhouette_scores - db_scores + ch_scores\n",
    "        best_model = models[np.argmax(combined_scores)]\n",
    "        \n",
    "        return best_model\n",
    "\n",
    "\n",
    "    def calculate_template_similarity(self, verbose=0):\n",
    "        pass\n",
    "    def merge_clusters_my_templates(self, verbose=0):\n",
    "        pass\n",
    "\n",
    "    def create_clusters(self, batch_idx, start, end, verbose=0):\n",
    "        features = self.model_trainer.get_features(start, end)\n",
    "        paths = list(features.keys())\n",
    "        reps = np.array(list(features.values()))\n",
    "\n",
    "        models = self.get_models()\n",
    "        best_model = self.find_best_model(models, reps)\n",
    "        labels = best_model.fit_predict(reps)\n",
    "\n",
    "\n",
    "\n",
    "        clusters = {}\n",
    "        for file, cluster_id in zip(paths, labels):\n",
    "            if cluster_id not in clusters:\n",
    "                clusters[cluster_id] = []\n",
    "            clusters[cluster_id].append(file)\n",
    "        clusters = list(clusters.values())\n",
    "\n",
    "\n",
    "        \n",
    "        write_clusters(clusters, batch_idx, self.result_container_folder, [], self.transfer, verbose=verbose-1)\n",
    "        if verbose > 0:\n",
    "            print(\"-\"*70)\n",
    "\n",
    "    def process(self, verbose=0):\n",
    "        self.model_trainer.train()\n",
    "\n",
    "        # creating result folder\n",
    "        if self.option != \"merge\":\n",
    "            if os.path.exists(self.result_container_folder) and not self.overwrite:\n",
    "                raise(OverwritePermissionException(\"Overwriting permission not granted to overwrite \" + self.result_container_folder))\n",
    "            else:\n",
    "                if os.path.exists(self.result_container_folder):\n",
    "                    shutil.rmtree(self.result_container_folder)\n",
    "                os.makedirs(self.result_container_folder)\n",
    "\n",
    "        if self.option != \"merge\":\n",
    "            for batch_idx, start in enumerate(range(0, len(self.model_trainer.dataset), self.batch_size)):\n",
    "                self.create_clusters(batch_idx, start, start + self.batch_size, verbose=self.verbose-1)\n",
    "\n",
    "            # if images are done in one batch terminate the code after organizing result folders\n",
    "            if self.batch_size >= len(self.model_trainer.dataset):\n",
    "                for file in os.listdir(self.result_container_folder):\n",
    "                    new_file_name = file.replace(\"batch_0\", \"result\")\n",
    "                    os.rename(os.path.join(self.result_container_folder, file), os.path.join(self.result_container_folder, new_file_name))\n",
    "                os.remove(os.path.join(self.result_container_folder, \"image_similarities_result.json\"))\n",
    "                print_verbose(\"f\", \"no merge needed to single batch\", self.verbose)\n",
    "            \n",
    "        if self.option == \"dontmerge\":\n",
    "            print_verbose(\"f\", \"finishing because of no merge request\", self.verbose)\n",
    "\n",
    "        # # gets each batchs folder\n",
    "        # batch_folder_paths = sorted([os.path.join(self.result_container_folder, f)\n",
    "        #                             for f in os.listdir(self.result_container_folder)\n",
    "        #                             if os.path.isdir(os.path.join(self.result_container_folder, f))])\n",
    "\n",
    "        # # merge each batch to get which clusters folders should be merged together\n",
    "        # template_cluster_folders_to_merge_list = self.merge_clusters_by_templates(batch_folder_paths, verbose=self.verbose-1)\n",
    "\n",
    "        # if self.option != \"merge\":\n",
    "        #     print_verbose(\"r\", str(len(template_cluster_folders_to_merge_list) - 1) + \" cluster found at result\", self.verbose)\n",
    "        # if self.option == \"merge\":\n",
    "        #     print_verbose(\"m\", str(len(template_cluster_folders_to_merge_list) - 1) + \" cluster found at result\", self.verbose)\n",
    "        \n",
    "        # # creating result folder and merging cluster folders\n",
    "        # result_folder_path = os.path.join(self.result_container_folder, \"results\")\n",
    "        # os.mkdir(result_folder_path)\n",
    "        # for e, template_cluster_folders_to_merge in enumerate(template_cluster_folders_to_merge_list):\n",
    "        #     cluster_folder_path = os.path.join(result_folder_path, \"cluster_\" + str(e))\n",
    "        #     if e == len(template_cluster_folders_to_merge_list) - 1:\n",
    "        #         cluster_folder_path = os.path.join(result_folder_path, \"outliers\")\n",
    "\n",
    "        #     os.mkdir(cluster_folder_path)\n",
    "        #     for template_cluster_folder in template_cluster_folders_to_merge:\n",
    "        #         for file in os.listdir(template_cluster_folder):\n",
    "        #             image_transfer(self.transfer, os.path.join(template_cluster_folder, file), os.path.join(cluster_folder_path, file))\n",
    "                    \n",
    "        # # removing unnecessary files and folders after merging results\n",
    "        # for folder in os.listdir(self.result_container_folder):\n",
    "        #     if folder != \"results\":\n",
    "        #         if os.path.isdir(os.path.join(self.result_container_folder, folder)):\n",
    "        #             shutil.rmtree(os.path.join(self.result_container_folder, folder))\n",
    "        #         if os.path.isfile(os.path.join(self.result_container_folder, folder)):\n",
    "        #             os.remove(os.path.join(self.result_container_folder, folder))\n",
    "\n",
    "\n",
    "    def __call__(self, verbose=0):\n",
    "        self.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check functions line by line\n",
    "# exceptions\n",
    "# batch size'ları karıştırma\n",
    "# checkpoint dosyalarını sil\n",
    "# verbose passes\n",
    "# if ckpt valid then no training at the beginning of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc = DL_Clustering(model_trainer=mt, method=\"kmeans\", batch_size=100, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "490-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

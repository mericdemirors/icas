{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/segment-anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Provide a Copy of the License: You must include a copy of the Apache License 2.0 in your project. This can be done by including a LICENSE file in the root of your project directory.\n",
    "\n",
    "2 - State Changes: If you modify any files from the original project, you need to include a prominent notice stating that you have changed the files. This helps in distinguishing your work from the original.\n",
    "\n",
    "3 - Preserve Notices: You must retain all the copyright, patent, trademark, and attribution notices from the source form of the original work in your derived work. This includes any notices contained in a NOTICE file if one exists.\n",
    "\n",
    "4 - Include the NOTICE File: If the original project includes a NOTICE file, you need to include a readable copy of the attribution notices contained within it in at least one of the following places: within a NOTICE text file distributed as part of your project, within the source code or documentation, or within a display generated by your project, if applicable.\n",
    "\n",
    "5 - Add Your Own Notices: You can add your own copyright statements and additional notices, as long as they do not contradict the terms of the Apache License 2.0.\n",
    "\n",
    "6 - State Compliance: You must state that your use of the original project's files is in compliance with the Apache License 2.0. This can typically be done in your project's documentation or README file.\n",
    "\n",
    "Here's a brief example of how you can include the Apache License 2.0 in your project:\n",
    "\n",
    "1 - LICENSE File: Create a LICENSE file in the root of your project directory with the full text of the Apache License 2.0.\n",
    "\n",
    "2 - Notice of Changes: If you modify any files, add a comment at the top of each modified file:  \n",
    "\\# Modified by [Your Name] on [Date]  \n",
    "\\# Original file available at [URL to original file]\n",
    "\n",
    "3 - Retain Notices: Ensure that all original notices are preserved in your project.\n",
    "\n",
    "4 - README or Documentation: Include a statement in your README file that your project includes files licensed under the Apache License 2.0 and provide a link to the license.\n",
    "\n",
    "By following these steps, you ensure that you are respecting the terms of the Apache License 2.0 while developing your pip library using the content from the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.morphology import flood_fill\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\" # not enough GPU memory\n",
    "\n",
    "from helper_exceptions import samPromptGenerationQuitException\n",
    "from segment_anything import SamPredictor, SamAutomaticMaskGenerator, sam_model_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class samSegmentator():\n",
    "    def __init__(self, sam, device:str=\"cpu\"):\n",
    "        \"\"\"initializing samSegmentator object\n",
    "        \"\"\"\n",
    "        self.sam = sam\n",
    "        self.device = device\n",
    "        self.sam_setted_image = None\n",
    "        self.DRAW_BG = {\"color\" : [0,0,255], \"val\" : 0} # right click\n",
    "        self.DRAW_FG = {\"color\" : [0,255,0], \"val\" : 1} # left click\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"resetting samSegmentator object variables\n",
    "        \"\"\"\n",
    "        self.paint_dict = None\n",
    "        self.clicked = False                # flag for drawing action\n",
    "        self.currently_drawing_rect = False # flag for rectangle action\n",
    "        self.display_rects = []             # selected rectangles for displaying\n",
    "        self.prompt_rects = []              # selected rectangles for prompting\n",
    "        self.prompt_coords = []             # selected coords for prompting\n",
    "        self.prompt_labels = []             # selected labels for prompting\n",
    "\n",
    "    def annotation_event_listener(self, event, x:int, y:int, flags, param):\n",
    "        \"\"\"mouse callbacks for annotation types\n",
    "\n",
    "        Args:\n",
    "            event (opencv event): mouse event to detect\n",
    "            x (int): column coordinate of mouse\n",
    "            y (int): row coordinate of mouse\n",
    "            flags (opencv flags): flags\n",
    "            param (dictionary): parameters\n",
    "        \"\"\"\n",
    "        # rectangle selection with middle button\n",
    "        if event == cv2.EVENT_MBUTTONDOWN:\n",
    "            self.currently_drawing_rect = True\n",
    "            self.ix, self.iy = x,y\n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if self.currently_drawing_rect:\n",
    "                self.image = self.altered.copy()\n",
    "                for r in self.display_rects:\n",
    "                    cv2.rectangle(self.image, (r[0], r[1]), (r[2], r[3]), [255,0,0], 1)\n",
    "                cv2.rectangle(self.image, (self.ix, self.iy), (x, y), [255,0,0], 1)\n",
    "        elif event == cv2.EVENT_MBUTTONUP:\n",
    "            self.currently_drawing_rect = False\n",
    "            for r in self.display_rects:\n",
    "                cv2.rectangle(self.altered, (r[0], r[1]), (r[2], r[3]), [255,0,0], 1)\n",
    "            cv2.rectangle(self.altered, (self.ix, self.iy), (x, y), [255,0,0], 1)\n",
    "            self.display_rects.append((self.ix, self.iy, x, y))\n",
    "            x = max(x, 0) # clips negatives to zero\n",
    "            x = min(max(x, 0), self.image.shape[1]) # clips out of bounds values to max value\n",
    "            y = max(y, 0) # clips negatives to zero\n",
    "            y = min(max(y, 0), self.image.shape[0]) # clips out of bounds values to max value\n",
    "            self.prompt_rects.append(np.array([self.ix, self.iy, x, y]))\n",
    "            self.altered = self.image.copy()\n",
    "        if self.currently_drawing_rect:\n",
    "            return\n",
    "\n",
    "        # annotation type selection with left/right click\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.clicked = True\n",
    "            self.paint_dict = self.DRAW_FG\n",
    "        elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "            self.clicked = True\n",
    "            self.paint_dict = self.DRAW_BG\n",
    "        elif self.clicked and (event == cv2.EVENT_LBUTTONUP or event == cv2.EVENT_RBUTTONUP):\n",
    "            self.clicked = False\n",
    "            cv2.circle(self.altered, (x,y), 3, self.paint_dict[\"color\"], -1)\n",
    "            self.prompt_coords.append([x,y])\n",
    "            self.prompt_labels.append(self.paint_dict[\"val\"])\n",
    "            self.image = self.altered.copy()\n",
    "   \n",
    "    def generate_prompt(self, image):\n",
    "        \"\"\"function to interactively generate sam prompt\n",
    "\n",
    "        Args:\n",
    "            image (numpy.ndarray): original image\n",
    "\n",
    "        Returns:\n",
    "            list: boxes, coords and labels for prompting\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "        self.original = image.copy()\n",
    "        self.image = self.original.copy()\n",
    "        self.altered = self.original.copy()\n",
    "\n",
    "        cv2.namedWindow(\"Annotations\", flags= cv2.WINDOW_AUTOSIZE | cv2.WINDOW_KEEPRATIO | cv2.WINDOW_GUI_NORMAL)\n",
    "        cv2.setMouseCallback(\"Annotations\", self.annotation_event_listener)\n",
    "        \n",
    "        while True:\n",
    "            cv2.imshow(\"Annotations\", self.image)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            # key bindings\n",
    "            if key == ord(\"q\"):\n",
    "                cv2.destroyWindow(\"annotation\")\n",
    "                raise(samPromptGenerationQuitException(\"samSegmentator received key q for quitting\"))\n",
    "            if key == ord(\" \"):\n",
    "                cv2.destroyWindow(\"Annotations\")\n",
    "                return self.prompt_rects, self.prompt_coords, self.prompt_labels\n",
    "            elif key == ord(\"r\"): # reset everything\n",
    "                self.reset()\n",
    "                self.image = self.altered = self.original.copy()\n",
    "\n",
    "    def get_label_from_sam_auto_output(self, sam_auto_output):\n",
    "        \"\"\"creates labeled image from sam output\n",
    "\n",
    "        Args:\n",
    "            sam_auto_output (list): list of informations about found masks\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: labeled image\n",
    "        \"\"\"\n",
    "        # get al masks and mark each of them with an unique id\n",
    "        masks = [x[\"segmentation\"] for x in sam_auto_output]\n",
    "        labeled_image = np.zeros(self.original.shape[:2], dtype=np.int16)\n",
    "        for e,mask in enumerate(masks):\n",
    "            labeled_image[mask] = e\n",
    "\n",
    "        # segment the unlabeled pixels\n",
    "        segment_pixels = np.where(labeled_image == 0)\n",
    "        segment_id = labeled_image.max()+1\n",
    "        while len(segment_pixels[0]) != 0: # while image has pixels with value 0 which means non-labeled segment\n",
    "            ri, ci = segment_pixels[0][0], segment_pixels[1][0] # get a segment pixel\n",
    "            \n",
    "            labeled_image = flood_fill(labeled_image, (ri, ci), segment_id, connectivity=1, in_place=True) # floodfill segment\n",
    "            extracted_segment = np.array(labeled_image == labeled_image[ri][ci]).astype(np.int16) # extract only segment as binary\n",
    "            extracted_segment = cv2.dilate(extracted_segment, np.ones((3,3)), iterations=1) # expand segment borders by one pixel to remove edges\n",
    "            np.putmask(labeled_image, extracted_segment != 0, segment_id) # overwrite expanded segment to labeled_image\n",
    "\n",
    "            segment_id = segment_id + 1\n",
    "            segment_pixels = np.where(labeled_image == 0)\n",
    "\n",
    "        return labeled_image\n",
    "    \n",
    "    def get_label_from_sam_with_prompt_output_mask(self, sam_with_prompt_output_mask):\n",
    "        \"\"\"creates labeled image from sam output\n",
    "\n",
    "        Args:\n",
    "            sam_with_prompt_output_mask (numpy.ndarray): binary merged mask from sam output\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: labeled image\n",
    "        \"\"\"\n",
    "        labeled_image = np.zeros(self.original.shape[:2], dtype=np.int16)\n",
    "        # mark masked pixels with -1\n",
    "        labeled_image[sam_with_prompt_output_mask] = -1\n",
    "\n",
    "        # label the masked pixels starting from 1\n",
    "        segment_pixels = np.where(labeled_image == -1)\n",
    "        segment_id = 1\n",
    "        while len(segment_pixels[0]) != 0: # while image has pixels with value 0 which means non-labeled segment\n",
    "            ri, ci = segment_pixels[0][0], segment_pixels[1][0] # get a segment pixel\n",
    "            \n",
    "            labeled_image = flood_fill(labeled_image, (ri, ci), segment_id, connectivity=1, in_place=True) # floodfill segment\n",
    "            extracted_segment = np.array(labeled_image == labeled_image[ri][ci]).astype(np.int16) # extract only segment as binary\n",
    "            extracted_segment = cv2.dilate(extracted_segment, np.ones((3,3)), iterations=1) # expand segment borders by one pixel to remove edges\n",
    "            np.putmask(labeled_image, extracted_segment != 0, segment_id) # overwrite expanded segment to labeled_image\n",
    "\n",
    "            segment_id = segment_id + 1\n",
    "            segment_pixels = np.where(labeled_image == -1)\n",
    "\n",
    "        # label the not masked pixels from last id\n",
    "        segment_pixels = np.where(labeled_image == 0)\n",
    "        while len(segment_pixels[0]) != 0: # while image has pixels with value 0 which means non-labeled segment\n",
    "            ri, ci = segment_pixels[0][0], segment_pixels[1][0] # get a segment pixel\n",
    "            \n",
    "            labeled_image = flood_fill(labeled_image, (ri, ci), segment_id, connectivity=1, in_place=True) # floodfill segment\n",
    "            extracted_segment = np.array(labeled_image == labeled_image[ri][ci]).astype(np.int16) # extract only segment as binary\n",
    "            extracted_segment = cv2.dilate(extracted_segment, np.ones((3,3)), iterations=1) # expand segment borders by one pixel to remove edges\n",
    "            np.putmask(labeled_image, extracted_segment != 0, segment_id) # overwrite expanded segment to labeled_image\n",
    "\n",
    "            segment_id = segment_id + 1\n",
    "            segment_pixels = np.where(labeled_image == 0)\n",
    "\n",
    "        return labeled_image\n",
    "\n",
    "    def sam_predict(self, image):\n",
    "        \"\"\"segmentation using sam model\n",
    "\n",
    "        Args:\n",
    "            image (numpy.ndarray): image to segment\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: segmented image\n",
    "        \"\"\"\n",
    "        self.original = image.copy()\n",
    "\n",
    "        if type(self.sam) == SamPredictor:\n",
    "            # prompt generation\n",
    "            prompt_rects, prompt_coords, prompt_labels = self.generate_prompt(image)\n",
    "\n",
    "            # assigning coords and labels to their boxes\n",
    "            # since multiple boxes and multiple coords/labels are not supported each box will be passed with\n",
    "            # its own related coords and labels individualy\n",
    "            coords_list = [np.array([pc for pc in prompt_coords \n",
    "                        if ((r[0]<=pc[0]<=r[2]) and (r[1]<=pc[1]<=r[3]))])for r in prompt_rects]\n",
    "            label_list = [np.array([prompt_labels[e] for e,pc in enumerate(prompt_coords)\n",
    "                        if ((r[0]<=pc[0]<=r[2]) and (r[1]<=pc[1]<=r[3]))]) for r in prompt_rects]\n",
    "\n",
    "            # set the image\n",
    "            if not np.array_equal(self.sam_setted_image, image):\n",
    "                self.sam.set_image(image)\n",
    "                self.sam_setted_image = image.copy()\n",
    "\n",
    "            # segment each pass individualy with its prompt\n",
    "            masks = []\n",
    "            for (c, l, b) in zip(coords_list, label_list, prompt_rects):\n",
    "                if len(c) == 0:\n",
    "                    c = None\n",
    "                if len(l) == 0:\n",
    "                    l = None\n",
    "                so = self.sam.predict(point_coords=c, point_labels=l, box=b)\n",
    "                masks.append(so[0][0])\n",
    "            # generate one mask and get labels\n",
    "            final_mask = np.logical_or.reduce(masks)\n",
    "            sam_segment = self.get_label_from_sam_with_prompt_output_mask(final_mask)\n",
    "        \n",
    "        elif type(self.sam) == SamAutomaticMaskGenerator:\n",
    "            # get mask and label the segments\n",
    "            sam_auto_output = self.sam.generate(image)\n",
    "            sam_segment = self.get_label_from_sam_auto_output(sam_auto_output)\n",
    "        \n",
    "        return sam_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/mericdemirors/Pictures/araba/araba.jpg\"\n",
    "image = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_config = sam_model_registry[\"vit_b\"](checkpoint=\"/home/mericdemirors/Downloads/sam_vit_b_01ec64.pth\").to(device)\n",
    "sam_with_prompt = SamPredictor(sam_config)\n",
    "sam_generator = samSegmentator(sam_with_prompt, device)\n",
    "segment = sam_generator.sam_predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_config = sam_model_registry[\"vit_b\"](checkpoint=\"/home/mericdemirors/Downloads/sam_vit_b_01ec64.pth\").to(device)\n",
    "sam_auto = SamAutomaticMaskGenerator(sam_config)\n",
    "sam_generator = samSegmentator(sam_auto, device)\n",
    "segment = sam_generator.sam_predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"u\", segment.astype(np.uint8)*(255//segment.max()))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "490-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
